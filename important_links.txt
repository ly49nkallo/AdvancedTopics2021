--- directly appliable ---

A pedagogical implementation of Autograd.
https://github.com/mattjj/autodidact/tree/master/autograd

NN example:
https://github.com/HIPS/autograd/blob/master/examples/neural_net.py

Optimizers (that actually do the training)
https://github.com/HIPS/autograd/blob/master/autograd/misc/optimizers.py

NN design:
https://hagan.okstate.edu/NNDesign.pdf

How backprop works for Convnets:
https://pavisj.medium.com/convolutions-and-backpropagations-46026a8f5d2c

Torch autograd
https://pytorch.org/docs/stable/autograd.html

Jacobian matrix and det
https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant

Homemade mlp demo:
https://nbviewer.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_demo.ipynb










--- less important stuff ---

einops uses
https://einops.rocks/pytorch-examples.html

lenia smooth life projects and home page
https://chakazul.github.io/lenia.html

a whole ton of pytorch gans
https://github.com/eriklindernoren/PyTorch-GAN/tree/master/implementations

Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
https://arxiv.org/abs/1511.06434

WGAN implementaiton
https://www.youtube.com/watch?v=pG0QZ7OddX4

cs231 lesson on optimization and Gradient Descent
https://cs231n.github.io/optimization-1/

SGD
https://en.wikipedia.org/wiki/Stochastic_gradient_descent